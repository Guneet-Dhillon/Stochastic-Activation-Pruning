{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Activation Pruning\n",
    "\n",
    "This tutorial walks through the implementation of Stochastic Activation Pruning (SAP), an algorithm proposed as a defense against adversarial attacks (fast gradient sign method ([FGSM](https://arxiv.org/pdf/1412.6572.pdf)) and iterative FGSM ([PGD](Towards deep learning models resistant to adversarial attacks)). It can be applied to pre-trained deep neural network models to increase robustness against small perturbations in input. It can also be applied on adversarially trained models to compound the benefits of the two techniques.\n",
    "\n",
    "The model in this tutorial follows the work described in the paper \n",
    "[Stochastic Activation Pruning for Robust Adversarial Defense](https://openreview.net/pdf?id=H1uR4GZRZ), written by Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Animashree Anandkumar.\n",
    "\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In order to run the code, AWS EC2 instances with GPUs can be used. They would already have the packages required to run the following code, but one change needs to be made.\n",
    "The following code clones my MXNet repo (https://github.com/Guneet-Dhillon/mxnet), forked from the original MXNet repo (https://github.com/apache/incubator-mxnet), but with the SAP method implemented. The code also builds the MXNet core shared library and installs the Python bindings.\n",
    "\n",
    "`git clone --recursive https://github.com/Guneet-Dhillon/mxnet.git`\n",
    "\n",
    "`cd mxnet`\n",
    "\n",
    "`make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1`\n",
    "\n",
    "`cd python`\n",
    "\n",
    "`pip3 install -e .`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm:\n",
    "\n",
    "Intuitively, the idea of SAP is \n",
    "to stochastically drop out activation nodes at each layer of deep network\n",
    "during forward propagation.\n",
    "We retain nodes with probabilities proportional \n",
    "to the magnitude of their activation \n",
    "and scale up the surviving nodes \n",
    "to preserve the dynamic range \n",
    "of the activations in each layer. \n",
    "Empirically, the approach preserves \n",
    "the accuracy of the original model.\n",
    "Notably, the method can be applied post-hoc \n",
    "to already-trained models.\n",
    "\n",
    "Formally, assume a given pretrained model, \n",
    "with activation layers (ReLU, Sigmoid, etc.) and input $x$ with its corresponding output $y$.\n",
    "For each of these layers, \n",
    "SAP converts the activation map\n",
    "to a multinomial distribution,\n",
    "choosing each activation\n",
    "with a probability proportional to its absolute value.\n",
    "In other words,\n",
    "we obtain the multinomial distribution\n",
    "of each activation layer \n",
    "with $L_1$ normalization of the absolute value of activations onto a $L_1$-ball simplex.\n",
    "Given the $i$'th layer activation map, \n",
    "$h^{i}\\in\\mathbb{R}^{a^{i}}$, \n",
    "the probability of sampling the $j$'th\n",
    "activation with value $(h^{i})_{j}$\n",
    "is given by\n",
    "$$\n",
    "p^{i}_{j}=\\frac{|(h^{i})_{j}|}{\\sum_{k=1}^{a^{i}}{|(h^{i})_{k}|}}.\n",
    "$$ \n",
    "We draw random samples with replacement \n",
    "from the activation map \n",
    "given the probability distribution described above.\n",
    "This makes it convenient to determine \n",
    "whether an activation would be sampled at all.\n",
    "If an activation is sampled, \n",
    "we scale it up by the inverse of the probability \n",
    "of sampling it over all the draws. \n",
    "If not, we set the activation to $0$. \n",
    "In this way, SAP preserves inverse propensity scoring of each activation.\n",
    "Under an instance $p$ of policy $\\pi$, \n",
    "we draw $r^{i}_{p}$ samples with replacement \n",
    "from this multinomial distribution.\n",
    "The new activation map, $M_{p}(h^{i})$ \n",
    "is given by\n",
    "$$\n",
    "M_{p}(h^{i})=h^{i}\\odot m^{i}_{p},   \\quad (m^{i}_{p})_{j}=\\frac{\\mathbb{I}((h^{i})_{j})}{1-(1-p^{i}_{j})^{r^{i}_{p}}},\n",
    "$$\n",
    "where $\\mathbb{I}((h^{i})_{j})$ \n",
    "is the indicator function\n",
    "that returns $1$ if $(h^{i})_{j}$ was sampled \n",
    "at least once,\n",
    "and $0$ otherwise.\n",
    "In this way, the model parameters\n",
    "are changed from $\\theta$ to $M_{p}(\\theta)$,\n",
    "for instance $p$ under policy $\\pi$, \n",
    "while the reweighting $1-(1-p^{i}_{j})^{r^{i}_{p}}$ preserves $\\mathbb{E}_{p\\sim\\pi}[M_{p}(h^{i})_{j}] =(h^{i})_{j}$.\n",
    "If the model was linear,\n",
    "the proposed pruning method\n",
    "would behave the same way as the original model\n",
    "in expectation.\n",
    "In practice, we find that even with the non-linearities in deep neural networks, for sufficiently many examples, \n",
    "SAP performs similarly to the un-pruned model.\n",
    "This guides our decision to apply SAP to pretrained models without performing fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage against Adversarial Attack:\n",
    "\n",
    "We attempt to explain \n",
    "the advantages of SAP\n",
    "under the assumption \n",
    "that we are applying it to a pre-trained model that achieves high generalization accuracy.\n",
    "For an instance $p$ of the policy $\\pi$, \n",
    "if the number of samples drawn for each layer $i$, $r^{i}_{p}$, is large, \n",
    "then fewer parameters of the neural network are pruned, \n",
    "and the scaling factor gets closer to $1$. \n",
    "Under this scenario, the stochastically pruned model \n",
    "performs almost identically to the original model. \n",
    "The stochasticity is not advantageous in this case, \n",
    "but there is no loss in accuracy in the pruned model \n",
    "as compared to the original model.\n",
    "\n",
    "On the other hand, with fewer samples in each layer, $r^{i}_{p}$, \n",
    "a large number of parameters of the neural network are pruned. \n",
    "Under this scenario, \n",
    "the SAP model's accuracy will drop \n",
    "compared to the original model's accuracy.\n",
    "But this model is stochastic and has more freedom to deceive the adversary.\n",
    "So the advantage of SAP comes if we can balance the number of samples drawn in a way that negligibly impacts accuracy \n",
    "but still confers robustness against adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Code:\n",
    "\n",
    "We will compare the accuracy of the original dense model and the SAP model on CIFAR-10, using a ResNet-20 architecture.\n",
    "\n",
    "We will need the following libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One can set the GPU IDs to be used for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpus = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "context = [mx.gpu(i) for i in gpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters that the user can tune for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# different levels of perturbation for the adversary (will be using FGSM to generate adversarial examples).\n",
    "epsilons = [0.0, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]\n",
    "# the fraction of samples to be drawn from each activation map for SAP\n",
    "frac = 1.0\n",
    "# the number of MC samples to estimate output of the model\n",
    "mc_samples_output = 100\n",
    "# the number of MC samples to estimate gradient of the model\n",
    "mc_samples_gradient = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, local_fname):\n",
    "    dir_name = os.path.dirname(local_fname)\n",
    "    if dir_name != \"\":\n",
    "        if not os.path.exists(dir_name):\n",
    "            try:\n",
    "                os.makedirs(dir_name)\n",
    "            except OSError as exc:\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "    r = requests.get(url, stream=True)\n",
    "    assert r.status_code == 200, \"failed to open %s\" % url\n",
    "    with open(local_fname, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "train_fname = './data/cifar10_train.rec'\n",
    "val_fname = './data/cifar10_val.rec'\n",
    "download_file('http://data.mxnet.io/data/cifar10/cifar10_train.rec', train_fname)\n",
    "download_file('http://data.mxnet.io/data/cifar10/cifar10_val.rec', val_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to obtain the data iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Uint8Iter(mx.io.DataIter):\n",
    "\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "\n",
    "    def next(self):\n",
    "        batch = self.iterator.next()\n",
    "        data = batch.data\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].astype('uint8').astype('float32')\n",
    "        return mx.io.DataBatch(data = data, label = batch.label)\n",
    "\n",
    "    def reset(self):\n",
    "        self.iterator.reset()\n",
    "\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return self.iterator.provide_data\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return self.iterator.provide_label\n",
    "\n",
    "def get_iter(batch_size):\n",
    "\n",
    "    # training iterator\n",
    "    train = mx.io.ImageRecordIter(\n",
    "        path_imgrec         = train_fname,\n",
    "        label_width         = 1,\n",
    "        mean_r              = 123.68,\n",
    "        mean_g              = 116.779,\n",
    "        mean_b              = 103.939,\n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax_label',\n",
    "        data_shape          = (3, 28, 28),\n",
    "        batch_size          = batch_size,\n",
    "        rand_crop           = 1,\n",
    "        max_random_scale    = 1,\n",
    "        pad                 = 4,\n",
    "        fill_value          = 127,\n",
    "        min_random_scale    = 1,\n",
    "        max_aspect_ratio    = 0,\n",
    "        random_h            = 36,\n",
    "        random_s            = 50,\n",
    "        random_l            = 50,\n",
    "        max_rotate_angle    = 0,\n",
    "        max_shear_ratio     = 0,\n",
    "        rand_mirror         = 1,\n",
    "        shuffle             = True)\n",
    "\n",
    "    # validation iterator\n",
    "    val = mx.io.ImageRecordIter(\n",
    "        path_imgrec         = val_fname,\n",
    "        label_width         = 1,\n",
    "        mean_r              = 123.68,\n",
    "        mean_g              = 116.779,\n",
    "        mean_b              = 103.939,\n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax_label',\n",
    "        batch_size          = batch_size,\n",
    "        data_shape          = (3, 28, 28),\n",
    "        rand_crop           = False,\n",
    "        rand_mirror         = False,\n",
    "        shuffle             = True)\n",
    "\n",
    "    return (Uint8Iter(train), Uint8Iter(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to obtain the symbol of the model. The original dense model will be the ResNet-20 architecture. In the case of SAP, each RELU activation map in that architecture will be appended by an SAP operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sap_unit(data, frac, data_shape):\n",
    "    shape = data.infer_shape(data=data_shape)[1][0]\n",
    "    act = mx.sym.flatten(data)\n",
    "    prob = mx.sym.broadcast_div(act, mx.sym.sum(act, axis=1).reshape((shape[0], 1)))\n",
    "    prune = mx.sym.StochasticActivationPruning(act, prob, frac=frac)\n",
    "    return prune.reshape(shape)\n",
    "\n",
    "def residual_unit(data, num_filter, stride, dim_match, name, typ='normal', frac=1.0, shape=()):\n",
    "    bn1 = mx.sym.BatchNorm(data=data, fix_gamma=False, momentum=0.9, eps=2e-5, name=name + '_bn1')\n",
    "    act1 = mx.sym.Activation(data=bn1, act_type='relu', name=name + '_act1')\n",
    "    if typ == 'sap':\n",
    "        act1 = sap_unit(act1, frac, shape)\n",
    "    conv1 = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(3, 3), stride=stride, pad=(1, 1), no_bias=True, workspace=256, name=name + '_conv1')\n",
    "    bn2 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, momentum=0.9, eps=2e-5, name=name + '_bn2')\n",
    "    act2 = mx.sym.Activation(data=bn2, act_type='relu', name=name+'_act2')\n",
    "    if typ == 'sap':\n",
    "        act2 = sap_unit(act2, frac, shape)\n",
    "    conv2 = mx.sym.Convolution(data=act2, num_filter=num_filter, kernel=(3, 3), stride=(1, 1), pad=(1, 1), no_bias=True, workspace=256, name=name + '_conv2')\n",
    "    if dim_match:\n",
    "        shortcut = data\n",
    "    else:\n",
    "        shortcut = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(1, 1), stride=stride, no_bias=True, workspace=256, name=name + '_sc')\n",
    "    return conv2 + shortcut\n",
    "\n",
    "def get_symbol(typ='dense', frac=1.0, shape=()):\n",
    "\n",
    "    filter_list = [16, 16, 32, 64]\n",
    "\n",
    "    data = mx.sym.Variable('data')\n",
    "    data = mx.sym.identity(data=data, name='id')\n",
    "    data = mx.sym.BatchNorm(data=data, fix_gamma=True, eps=2e-5, momentum=0.9, name='bn_data')\n",
    "    body = mx.sym.Convolution(data=data, num_filter=filter_list[0], kernel=(3, 3), stride=(1, 1), pad=(1, 1), no_bias=True, workspace=256, name='conv0')\n",
    "\n",
    "    for i in range(3):\n",
    "        body = residual_unit(body, filter_list[i+1], (1 if i==0 else 2, 1 if i==0 else 2), False, 'stage%d_unit%d' % (i + 1, 1), typ, frac, shape)\n",
    "        for j in range(2):\n",
    "            body = residual_unit(body, filter_list[i+1], (1, 1), True, 'stage%d_unit%d' % (i + 1, j + 2), typ, frac, shape)\n",
    "\n",
    "    bn1 = mx.sym.BatchNorm(data=body, fix_gamma=False, eps=2e-5, momentum=0.9, name='bn1')\n",
    "    relu1 = mx.sym.Activation(data=bn1, act_type='relu', name='relu1')\n",
    "    if typ == 'sap':\n",
    "        relu1 = sap_unit(relu1, frac, shape)\n",
    "    pool1 = mx.sym.Pooling(data=relu1, global_pool=True, kernel=(7, 7), pool_type='avg', name='pool1')\n",
    "    flat = mx.sym.Flatten(data=pool1)\n",
    "    fc1 = mx.sym.FullyConnected(data=flat, num_hidden=10, name='fc1')\n",
    "    \n",
    "    return mx.sym.SoftmaxOutput(data=fc1, name='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the original dense model. This is done using SGD, with minibatches of size 512, momentum\n",
    "of 0.9, weight decay of 0.0001, and a learning rate of 0.5 for the first 100 epochs, then 0.05 for the\n",
    "next 30 epochs, and then 0.005 for the next 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed paramters\n",
    "label_name = 'softmax_label'\n",
    "eval_metric = 'acc'\n",
    "lr = 0.5\n",
    "num_epoch = 150\n",
    "batch_size = 512\n",
    "lr_steps = [100, 130]\n",
    "lr_factor = 0.1\n",
    "batches_per_epoch = math.ceil(50000.0 / batch_size)\n",
    "mom = 0.9\n",
    "wd = 0.0001\n",
    "initializer = mx.init.Xavier(rnd_type='gaussian', factor_type='in', magnitude=2)\n",
    "disp_batches = 500\n",
    "\n",
    "# get data iterators\n",
    "train, val = get_iter(batch_size)\n",
    "\n",
    "# train model\n",
    "sym = get_symbol()\n",
    "lr_steps = [batches_per_epoch * i for i in lr_steps]\n",
    "lr_scheduler = mx.lr_scheduler.MultiFactorScheduler(step=lr_steps, factor=lr_factor)\n",
    "optimizer_params = {\n",
    "    'learning_rate' : lr,\n",
    "    'lr_scheduler'  : lr_scheduler,\n",
    "    'momentum'      : mom,\n",
    "    'wd'            : wd}\n",
    "batch_end_callbacks = [mx.callback.Speedometer(batch_size, disp_batches)]\n",
    "mod = mx.mod.Module(symbol=sym, context=context, label_names=[label_name,])\n",
    "mod.fit(train,\n",
    "    num_epoch               = num_epoch,\n",
    "    eval_data               = val,\n",
    "    eval_metric             = eval_metric,\n",
    "    optimizer_params        = optimizer_params,\n",
    "    initializer             = initializer,\n",
    "    batch_end_callback      = batch_end_callbacks,\n",
    "    allow_missing           = True)\n",
    "\n",
    "# save model\n",
    "mod.save_params('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an adversarial iterator that computes adversarial examples using the FGSM method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdversaryIter(mx.io.DataIter):\n",
    "\n",
    "    def __init__(self, iterator, batch_size, mod, epsilons, frac):\n",
    "        self.iterator = iterator\n",
    "        self.batch_size = batch_size\n",
    "        self.mod = mod\n",
    "        self.epsilons = epsilons\n",
    "        self.frac = frac\n",
    "\n",
    "    def next(self):\n",
    "        batch = self.iterator.next()\n",
    "        data = batch.data\n",
    "        self.mod.forward(batch)\n",
    "        self.mod.backward()\n",
    "        grad = self.mod.get_input_grads()\n",
    "        for i in range(mc_samples_gradient - 1):\n",
    "            self.mod.forward(batch)\n",
    "            self.mod.backward()\n",
    "            new_grad = self.mod.get_input_grads()\n",
    "            for j in range(len(data)):\n",
    "                grad[j] += new_grad[j]\n",
    "        for i in range(len(data)):\n",
    "            grad[i] /= float(mc_samples_gradient)\n",
    "        new_data = [[matrix.copy() for matrix in data] for i in range(len(self.epsilons))]\n",
    "        for i in range(len(self.epsilons)):\n",
    "            for j in range(len(data)):\n",
    "                noise = self.epsilons[i] * mx.nd.sign(grad[j].as_in_context(new_data[i][j].context))\n",
    "                new_data[i][j] += noise\n",
    "                new_data[i][j] = mx.nd.clip(new_data[i][j], 0, 255)\n",
    "        return [mx.io.DataBatch(data = new_data[i], label = batch.label) for i in range(len(self.epsilons))]\n",
    "\n",
    "    def reset(self):\n",
    "        self.iterator.reset()\n",
    "\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return self.iterator.provide_data\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return self.iterator.provide_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of the original dense model and SAP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed parameters\n",
    "label_name = 'softmax_label'\n",
    "eval_metric = 'acc'\n",
    "batch_size = 500 * len(gpus)\n",
    "shape = (500, 3, 28, 28)\n",
    "\n",
    "# get data iterators\n",
    "train, val = get_iter(batch_size)\n",
    "\n",
    "def accuracy(iterator, mod, number):\n",
    "\n",
    "    correct = [0.0 for i in range(number)]\n",
    "    total = [0.0 for i in range(number)]\n",
    "\n",
    "    iterator.reset()\n",
    "    while True:\n",
    "        try:\n",
    "            batches = iterator.next()\n",
    "        except StopIteration:\n",
    "            break\n",
    "        for j in range(number):\n",
    "            batch = batches[j]\n",
    "\n",
    "            data = batch.data\n",
    "            mod.forward(batch)\n",
    "            out = mod.get_outputs()[0].asnumpy()\n",
    "            for i in range(mc_samples_output - 1):\n",
    "                mod.forward(batch)\n",
    "                out += mod.get_outputs()[0].asnumpy()\n",
    "            out = out / float(mc_samples_output)\n",
    "\n",
    "            label = batch.label[0].asnumpy()\n",
    "\n",
    "            for i in range(out.shape[0]):\n",
    "                index = np.argmax(out[i])\n",
    "                total[j] += 1.0\n",
    "                if index == label[i]:\n",
    "                    correct[j] += 1.0\n",
    "\n",
    "    acc = [0 for i in range(number)]\n",
    "    for i in range(number):\n",
    "        acc[i] = correct[i] / total[i]\n",
    "\n",
    "    return acc\n",
    "\n",
    "def validate(typ):\n",
    "\n",
    "    # get model\n",
    "    sym = get_symbol(typ, frac, shape)\n",
    "    mod = mx.mod.Module(symbol=sym, context=context, label_names=[label_name,])\n",
    "    mod.bind(val.provide_data, label_shapes = val.provide_label, for_training = False)\n",
    "    mod.load_params('trained_model')\n",
    "\n",
    "    # get model for adversarial examples\n",
    "    adv_sym = get_symbol(typ, frac, shape)\n",
    "    mod_adv = mx.mod.Module(symbol=adv_sym, context=context, label_names=[label_name,])\n",
    "    mod_adv.bind(val.provide_data, label_shapes = val.provide_label, for_training = True, inputs_need_grad = True)\n",
    "    mod_adv.load_params('trained_model')\n",
    "\n",
    "    # validate model\n",
    "    val.reset()\n",
    "    new_val = AdversaryIter(val, batch_size, mod_adv, epsilons, frac)\n",
    "    acc = accuracy(new_val, mod, len(epsilons))\n",
    "\n",
    "    return acc\n",
    "\n",
    "dense_acc = validate('dense')\n",
    "sap_acc = validate('sap')\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "x = range(8)\n",
    "ax.plot(range(8), dense_acc, 'o', linestyle='-', c='k', label='DENSE')\n",
    "ax.plot(range(8), sap_acc, 'o', linestyle='-', c='r', label='SAP-$100$')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.xticks(range(8), [0, 1, 2, 4, 8, 16, 32, 64])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.2, box.width, box.height * 0.8])\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.10), fancybox=True, shadow=True, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
